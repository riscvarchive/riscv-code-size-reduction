= Zero page relocation

[NOTE]

  This proposed is entirely based on David Horner's work, written up by Tariq
  
This proposal adds new CSRs to control the behaviour of unusual encodings, which are not very useful in general, and changes the behaviour to make it more useful.


== zero page JALR

=== CSR

* `MZPJALR[0] = enable`
* `MZPJALR[31:12] = base`
* `MZPJALR[11:1]` - *reserved*

[NOTE]

  The https://github.com/riscv/riscv-code-size-reduction/blob/master/ISA%20proposals/Huawei/table%20jump.adoc[table jump proposal] reduces the usefulness of this

== Behaviour

If `MZPJALR.enable=0` then the behaviour of `JALR rd, offset(x0)` is unchanged.

If `MZPJALR.enable=1` then the behaviour of `JALR rd, offset(x0)` takes on a new meaning. 

. `MZPJALR.base` is substituted for `x0`.
. `offset` is scaled by `XLEN/8` before use to give a bigger range.

Therefore the behaviour is:

[source,sourceCode,text]
----

jalr rd, offset(x0);# executes as jalr rd, MZPJALR.base+offset*(XLEN/8)

----

This gives a 16KB region which can always be accessed by `jalr` from anywhere in the address map. Note that there is no 16-bit form as `x0` cannot be specified as the base register for `c.jalr`.

== Zero Page load/store

=== CSR

* `MZPLDST[0] = enable`
* `MZPLDST[31:12] = base`
* `MZPLDST[11:1]` - *reserved*

== Behaviour

If `MZPLDST.enable=0` then the behaviour of `[lq|ld|ldu|lw|lwu|lh|lhu|lb|lbu] rd, offset(x0)` is unchanged.

If `MZPLDST.enable=0` then the behaviour of `[sq|sd|sw|sh|sb] rs2, offset(x0)` is unchanged.

If `MZPLDST.enable=1` then the behaviour of `[lq|ld|ldu|lw|lwu|lh|lhu|lb|lbu] rd, offset(x0)` takes on a new meaning.

If `MZPLDST.enable=1` then the behaviour of `[sq|sd|sw|sh|sb] rs2, offset(x0)` takes on a new meaning.

. `MZPLDST.base` is substituted for `x0`.
. `offset` *may be* shifted left by the access width before use, to give a bigger range TBD.

Therefore the behaviour is:

[source,sourceCode,text]
----

lb[u] rd, offset(x0);# executes as lb[u] rd, MZPLDST.base+offset
lh[u] rd, offset(x0);# executes as lh[u] rd, MZPLDST.base+offset*2
lw[u] rd, offset(x0);# executes as lw[u] rd, MZPLDST.base+offset*4
ld[u] rd, offset(x0);# executes as ld[u] rd, MZPLDST.base+offset*8
lq    rd, offset(x0);# executes as lq    rd, MZPLDST.base+offset*16

sb rs1, offset(x0);# executes as sb rs1, MZPLDST.base+offset
sh rs1, offset(x0);# executes as sh rs1, MZPLDST.base+offset*2
sw rs1, offset(x0);# executes as sw rs1, MZPLDST.base+offset*4
sd rs1, offset(x0);# executes as sd rs1, MZPLDST.base+offset*8
sq rs1, offset(x0);# executes as sq rs1, MZPLDST.base+offset*16

----

== Address ranges

It's possible to have a variable range depending upon the access with giving a 4KB(`lb[u]/sb)`-64KB(`lq/sq`) region which can always be accessed by loads and stores from anywhere in the address map without using the global pointer, although the whole range can't be accessed with the all data widths. 

Alternatives are:

. use the offset directly, like normal load/stores
. make the offset scaling programmable, adding a field into `MZPLDST` to control it
.. then the disassembler should probably know about it
. scale the offset by `XLEN/8`, and so use it for naturally aligned data types only
.. sub-word loads become awkward, sub-word stores cannot be issued without copying `MZPLDST.base` into a normal register, unless an RMW sequence is issued
.. but *most* load/stores into a data area fit the machine width, so it's a case of using it carefully
.. this also means that the disassembler will know the scaling as it is fixed based upon the architecture

== Application

If compiling with the GCC option `-fstack-protector-strong` then every function in the Huawei IoT code has these:


[source,sourceCode,text]
----
 e04a5e:  00f00437                lui     s0,0xf00
 e04a62:  02c42783                lw      a5,44(s0) # f0002c <__stack_chk_guard>
----

Some functions also have this (sometimes it's a 32-bit sequence to call it)

[source,sourceCode,text]
----
10bef2c:     ffd47097                auipc   ra,0xffd47
10bef30:     f52080e7                jalr    -174(ra) # e05e7e <__stack_chk_fail>
----

These could be replaced by zero-page `jalr` and `lw` meaning that 64-bit sequences would never be required. Additionally table jump can be used for the calls to `__stack_chk_fail`

== Link Time Optimisation

The linker should be able to make use of this feature, so the compiler doesn't need to know about it.

== Context save/restore

Whether `MZPJALR/MZPLDST` are saved and restored on a context switch is platform defined. They could be set globally for all contexts to use, or they could be set separately.

It is probably not worth using the CSR space to create supervisor mode versions, as this scheme is primarily targetting small embedded cores which only supoprt M/U-modes. TBD.

== Virtual memory and PMP

The resulting address from the zero page instructions are virtual addresses where virtual memory is implemented and enabled. 

They are also subject to PMP checks, where the PMP is implemented and enabled.

Therefore the generated addresses are handled identically to any other load/store or fetch addresses.

== Disassembly

The disassembly for this should be modified to make it clear that zero page mode is in use. For example:

[source,sourceCode,text]
----
lw a1, 0x100(x0)
----

maybe should disassemble as

[source,sourceCode,text]
----
lw a1, 0x100(zp)
----

to show it's relative to the zero-page pointer, or maybe use a different mnemonic:

[source,sourceCode,text]
----
zlw a1, 0x100
----

== Caveat

If a SoC has memory allocated Â±2KB around address zero (i.e. the bottom and top 2KB of the address map), and the compiler / handwritten assembler reference it by using load/stores/`JALR` which reference x0 then the zero page mode will not be usable on that platform.




